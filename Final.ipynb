{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e000c6-34e6-4a54-b7dc-65aef508ac28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.signal import iirfilter, sosfreqz, sosfilt\n",
    "import librosa\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as lib\n",
    "import librosa.display\n",
    "import os\n",
    "from IPython.display import Audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e13b5-6c75-46d4-99b9-5e2a72b62e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.signal import iirfilter, sosfreqz, sosfilt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as lib\n",
    "import os\n",
    "from IPython.display import Audio\n",
    "\n",
    "def merge_audios(audio_files):\n",
    "    audios = [lib.load(file, sr=None)[0] for file in audio_files]\n",
    "    return np.concatenate(audios)\n",
    "\n",
    "def plot_audio(y, sr, title=\"\"):\n",
    "    plt.figure(figsize=(25, 6)) \n",
    "    \n",
    "    # Waveform\n",
    "    plt.subplot(1, 3, 1)\n",
    "    lib.display.waveshow(y, sr=sr)\n",
    "    plt.title('Waveform - ' + title)\n",
    "    \n",
    "    # Spectrogram\n",
    "    plt.subplot(1, 3, 2)\n",
    "    D = lib.amplitude_to_db(np.abs(lib.stft(y)), ref=np.max)\n",
    "    lib.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "    plt.colorbar(label='dB')\n",
    "    plt.title('Spectrogram - ' + title)\n",
    "    \n",
    "    # MFCC\n",
    "    plt.subplot(1, 3, 3)\n",
    "    M = lib.feature.mfcc(y=y, sr=sr)\n",
    "    lib.display.specshow(M, sr=sr, x_axis='time')\n",
    "    plt.colorbar()\n",
    "    plt.title('MFCC - ' + title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "data_raw = []\n",
    "data_processed = []\n",
    "\n",
    "base_path = \"16000_pcm_speeches\"\n",
    "speakers = [\"Benjamin_Netanyau\", \"Jens_Stoltenberg\", \"Julia_Gillard\", \"Magaret_Tarcher\", \"Nelson_Mandela\"]\n",
    "\n",
    "def merge_audios(audio_files):\n",
    "    audios = [lib.load(file, sr=None)[0] for file in audio_files]\n",
    "    return np.concatenate(audios)\n",
    "\n",
    "def denoise_method_d1(y):\n",
    "    D = lib.stft(y)\n",
    "    magnitude, phase = lib.magphase(D)\n",
    "    magnitude_denoised = lib.decompose.nn_filter(magnitude, aggregate=np.median, metric='cosine')\n",
    "    y_denoised = lib.istft(magnitude_denoised * phase)\n",
    "    return y_denoised\n",
    "\n",
    "def denoise_method_spectral(y, sr):\n",
    "    # Calculate spectral centroid\n",
    "    cent = np.mean(lib.feature.spectral_centroid(y=y, sr=sr))\n",
    "    \n",
    "    # Low-shelf\n",
    "    sos = iirfilter(N=2, Wn=cent/sr, btype='low', ftype='butter', output='sos')\n",
    "    y_low_shelf = sosfilt(sos, y)\n",
    "    \n",
    "    # High-shelf\n",
    "    sos = iirfilter(N=2, Wn=cent/sr, btype='high', ftype='butter', output='sos')\n",
    "    y_high_shelf = sosfilt(sos, y_low_shelf)\n",
    "    \n",
    "    # Amplify signal to compensate for volume reduction\n",
    "    y_denoised = y_high_shelf * 10\n",
    "    return y_denoised\n",
    "\n",
    "\n",
    "for speaker in speakers:\n",
    "    speaker_path = os.path.join(base_path, speaker)\n",
    "    audio_files = [os.path.join(speaker_path, f) for f in sorted(os.listdir(speaker_path))]\n",
    "    \n",
    "    y, sr = merge_audios(audio_files[:10]), 16000\n",
    "\n",
    "    # Visualize raw audio\n",
    "    display(Audio(y, rate=sr))\n",
    "    plot_audio(y, sr, title=\"Raw Audio\")\n",
    "\n",
    "    # Trimming\n",
    "    y_trimmed, _ = lib.effects.trim(y, top_db=20)\n",
    "    display(Audio(y_trimmed, rate=sr))\n",
    "    plot_audio(y_trimmed, sr, title=\"Trimmed Audio\")\n",
    "\n",
    "    # Removing silence\n",
    "    y_no_silence = [y_trimmed[start:end] for start, end in lib.effects.split(y_trimmed, top_db=20)]\n",
    "    y_combined = np.concatenate(y_no_silence)\n",
    "    display(Audio(y_combined, rate=sr))\n",
    "    plot_audio(y_combined, sr, title=\"Audio without Silence\")\n",
    "\n",
    "    # Denoising using D1 method\n",
    "    y_denoised_d1 = denoise_method_d1(y_combined)\n",
    "    display(Audio(y_denoised_d1, rate=sr))\n",
    "    plot_audio(y_denoised_d1, sr, title=\"Denoised Audio (D1 Method)\")\n",
    "\n",
    "    # Denoising using spectral method\n",
    "    y_denoised_spectral = denoise_method_spectral(y_combined, sr)\n",
    "    display(Audio(y_denoised_spectral, rate=sr))\n",
    "    plot_audio(y_denoised_spectral, sr, title=\"Denoised Audio (Spectral Method)\")\n",
    "    \n",
    "     # Extracting MFCCs for raw audio\n",
    "    mfcc_raw = lib.feature.mfcc(y=y_combined, sr=sr, n_mfcc=20)\n",
    "    mean_mfcc_raw = mfcc_raw.mean(axis=1)\n",
    "    data_raw.append([*mean_mfcc_raw, speaker])\n",
    "    \n",
    "    # Extracting MFCCs for processed (denoised) audio\n",
    "    mfcc_processed = lib.feature.mfcc(y=y_denoised_spectral, sr=sr, n_mfcc=20)\n",
    "    mean_mfcc_processed = mfcc_processed.mean(axis=1)\n",
    "    data_processed.append([*mean_mfcc_processed, speaker])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#Histogram for number of entries \n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.countplot(data=mfd, x='Label', palette='viridis') \n",
    "plt.title('Number of audio files for Each Speaker', fontsize=16)\n",
    "plt.xlabel('Speaker', fontsize=14)\n",
    "plt.ylabel('Number of entries', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks([i for i in range(0,1501,100)])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Raw Audio MFCCs\n",
    "df_raw = pd.DataFrame(data_raw, columns=[f\"{i+1}\" for i in range(20)] + ['Label'])\n",
    "mean_mfccs_raw = df_raw.groupby('Label').mean()\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(mean_mfccs_raw, annot=True, cmap=\"YlGnBu\", linewidths=.5)\n",
    "plt.title(\"Mean MFCCs for Raw Audio of each speaker\")\n",
    "plt.show()\n",
    "\n",
    "# Processed Audio MFCCs\n",
    "df_processed = pd.DataFrame(data_processed, columns=[f\"{i+1}\" for i in range(20)] + ['Label'])\n",
    "mean_mfccs_processed = df_processed.groupby('Label').mean()\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(mean_mfccs_processed, annot=True, cmap=\"YlGnBu\", linewidths=.5)\n",
    "plt.title(\"Mean MFCCs for Processed Audio of each speaker\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation Matrix for Raw Audio MFCCs\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df_raw.iloc[:, :-1].corr(), annot=True, cmap=\"coolwarm\", linewidths=.5)\n",
    "plt.title(\"MFCCs Correlation Matrix for Raw Audio\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation Matrix for Processed Audio MFCCs\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df_processed.iloc[:, :-1].corr(), annot=True, cmap=\"coolwarm\", linewidths=.5)\n",
    "plt.title(\"MFCCs Correlation Matrix for Processed Audio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcedabb-eecd-41f9-94ca-e76a674e79de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"16000_pcm_speeches/\"\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07e1078-70cc-4750-9602-1294b06fbafc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_wav_paths(speaker):\n",
    "    speaker_path = data_dir + speaker\n",
    "    all_paths = [item for item in os.listdir(speaker_path)]\n",
    "    return all_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eaa003-d8ab-4c5e-9d4f-c1ff76b0ac0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nelson_mandela_paths = get_wav_paths(\"Nelson_Mandela\")\n",
    "margaret_thatcher_paths = get_wav_paths(\"Magaret_Tarcher\")\n",
    "benjamin_netanyau_paths = get_wav_paths(\"Benjamin_Netanyau\")\n",
    "jens_stoltenberg_paths = get_wav_paths( 'Jens_Stoltenberg')\n",
    "julia_gillard_paths = get_wav_paths(\"Julia_Gillard\")\n",
    "\n",
    "noise1_paths = get_wav_paths(\"_background_noise_\")\n",
    "noise2_paths = get_wav_paths(\"other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4878d654-1974-4359-a362-eb88d2fa9937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_wav(wav_path, speaker, sample_rate=16000):\n",
    "    wav_path = data_dir + speaker + \"/\" + wav_path\n",
    "    wav_data, _ = librosa.load(wav_path, sr=sample_rate, mono=True)\n",
    "    wav_data = np.reshape(wav_data, (1, -1))\n",
    "    return wav_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dea697-9c30-4f84-a5d2-a0c3b5d50ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_training_data(speaker_paths, speaker, label):\n",
    "    wavs, labels = [], []\n",
    "    for i in tqdm(speaker_paths):\n",
    "        wav = load_wav(i, speaker)\n",
    "        wavs.append(wav)\n",
    "        labels.append(label)\n",
    "    return wavs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1d15eb-e326-4940-be16-7be908337dde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nelson_mandela_wavs, nelson_mandela_labels = generate_training_data(nelson_mandela_paths, \"Nelson_Mandela\", 0) \n",
    "margaret_thatcher_wavs, margaret_thatcher_labels = generate_training_data(margaret_thatcher_paths, \"Magaret_Tarcher\", 1) \n",
    "benjamin_netanyau_wavs, benjamin_netanyau_labels = generate_training_data(benjamin_netanyau_paths, \"Benjamin_Netanyau\", 2) \n",
    "jens_stoltenberg_wavs, jens_stoltenberg_labels = generate_training_data(jens_stoltenberg_paths, \"Jens_Stoltenberg\", 3) \n",
    "julia_gillard_wavs, julia_gillard_labels = generate_training_data(julia_gillard_paths, \"Julia_Gillard\", 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e86cb3-c4e1-48f1-ab31-698931f191a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "julia_gillard_labels = julia_gillard_labels[1:]\n",
    "julia_gillard_wavs = julia_gillard_wavs[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4cfe27-ad25-47ca-ad81-fab53a39ff32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_wavs = nelson_mandela_wavs + margaret_thatcher_wavs + benjamin_netanyau_wavs + jens_stoltenberg_wavs + julia_gillard_wavs\n",
    "all_labels = nelson_mandela_labels + margaret_thatcher_labels + benjamin_netanyau_labels + jens_stoltenberg_labels + julia_gillard_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00825a1f-7585-4d14-81f7-4d721b4c442e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def merge_audios(audio_files):\n",
    "    audios = [lib.load(file, sr=None)[0] for file in audio_files]\n",
    "    return np.concatenate(audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da82b31-cc1f-4dfb-bb8f-e07024c386ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "data_raw = []\n",
    "data_processed = []\n",
    "\n",
    "base_path = \"16000_pcm_speeches\"\n",
    "speakers = [\"Benjamin_Netanyau\", \"Jens_Stoltenberg\", \"Julia_Gillard\", \"Magaret_Tarcher\", \"Nelson_Mandela\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5375c3ff-c9b9-4235-83a1-fdff82db3f3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_audios(audio_files):\n",
    "    audios = [lib.load(file, sr=None)[0] for file in audio_files]\n",
    "    return np.concatenate(audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c627fb9-9273-4e8f-afa7-3899f48db005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for speaker in speakers:\n",
    "    speaker_path = os.path.join(base_path, speaker)\n",
    "    audio_files = [f for f in os.listdir(speaker_path) if f.endswith(\".wav\")]\n",
    "    num_audio_files = len(audio_files)\n",
    "    data_raw.append([speaker, num_audio_files])\n",
    "\n",
    "df_speaker_counts = pd.DataFrame(data_raw, columns=['Speaker', 'Count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f07b5-ef29-43c7-ad48-b664f80f4b38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#histogram\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.barplot(data=df_speaker_counts, x='Speaker', y='Count', palette='viridis')\n",
    "plt.title('Number of Audio Files for Each Speaker', fontsize=16)\n",
    "plt.xlabel('Speaker', fontsize=14)\n",
    "plt.ylabel('Number of Entries', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab192083-9447-49e0-8e52-7081a1e6d6fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_audio_wave(y, sr, title=\"\"):\n",
    "    plt.figure(figsize=(10, 4)) \n",
    "    lib.display.waveshow(y, sr=sr)\n",
    "    plt.title('Waveform - ' + title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_audio_wave_spect(y, sr, title=\"\"):\n",
    "    plt.figure(figsize=(10, 4))    \n",
    "    # Spectrogram\n",
    "    D = lib.amplitude_to_db(np.abs(lib.stft(y)), ref=np.max)\n",
    "    lib.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "    plt.colorbar(label='dB')\n",
    "    plt.title('Spectrogram - ' + title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_audio_wave_mfcc(y, sr, title=\"\"):\n",
    "    plt.figure(figsize=(10, 4)) \n",
    "    # MFCC\n",
    "    M = lib.feature.mfcc(y=y, sr=sr)\n",
    "    lib.display.specshow(M, sr=sr, x_axis='time')\n",
    "    plt.colorbar()\n",
    "    plt.title('MFCC - ' + title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4383d3-cffb-4df9-b670-7f5613b51b44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_audios(audio_files):\n",
    "    audios = [lib.load(file, sr=None)[0] for file in audio_files]\n",
    "    return np.concatenate(audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9df1f-76dc-45aa-a44f-86550d7cc6b2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to denoise audio using D1 method\n",
    "def denoise_method_d1(y):\n",
    "    D = lib.stft(y)\n",
    "    magnitude, phase = lib.magphase(D)\n",
    "    magnitude_denoised = lib.decompose.nn_filter(magnitude, aggregate=np.median, metric='cosine')\n",
    "    y_denoised = lib.istft(magnitude_denoised * phase)\n",
    "    return y_denoised\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf408f8-6989-49dc-85f5-8e2a6d753f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to denoise audio using spectral method\n",
    "def denoise_method_spectral(y, sr):\n",
    "    # Calculate spectral centroid\n",
    "    cent = np.mean(lib.feature.spectral_centroid(y=y, sr=sr))\n",
    "\n",
    "    # Low-shelf\n",
    "    sos = iirfilter(N=2, Wn=cent/sr, btype='low', ftype='butter', output='sos')\n",
    "    y_low_shelf = sosfilt(sos, y)\n",
    "\n",
    "    # High-shelf\n",
    "    sos = iirfilter(N=2, Wn=cent/sr, btype='high', ftype='butter', output='sos')\n",
    "    y_high_shelf = sosfilt(sos, y_low_shelf)\n",
    "\n",
    "    # Amplify signal to compensate for volume reduction\n",
    "    y_denoised = y_high_shelf * 10\n",
    "    return y_denoised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551b3102-d01d-4ed5-90f9-34c57b77ef95",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for speaker in speakers:\n",
    "    speaker_path = os.path.join(base_path, speaker)\n",
    "    audio_files = [os.path.join(speaker_path, f) for f in sorted(os.listdir(speaker_path))]\n",
    "    y, sr = merge_audios(audio_files[:10]), 16000\n",
    "\n",
    "    print(f\"Processing audio from class: {speaker}\")\n",
    "    # Visualize raw audio\n",
    "    display(Audio(data=y, rate=sr))\n",
    "    plot_audio_wave(y, sr, title=\"Raw Audio\")\n",
    "    plot_audio_wave_spect(y, sr, title=\"Raw Audio\")\n",
    "    plot_audio_wave_mfcc(y, sr, title=\"Raw Audio\")\n",
    "\n",
    "    # Trimming\n",
    "    y_trimmed, _ = lib.effects.trim(y, top_db=20)\n",
    "    display(Audio(y_trimmed, rate=sr))\n",
    "    plot_audio_wave(y_trimmed, sr, title=\"Trimmed Audio\")\n",
    "    plot_audio_wave_spect(y_trimmed, sr, title=\"Trimmed Audio\")\n",
    "    plot_audio_wave_mfcc(y_trimmed, sr, title=\"Trimmed Audio\")\n",
    "\n",
    "    # Removing silence\n",
    "    y_no_silence = [y_trimmed[start:end] for start, end in lib.effects.split(y_trimmed, top_db=20)]\n",
    "    y_combined = np.concatenate(y_no_silence)\n",
    "    display(Audio(y_combined, rate=sr))\n",
    "    plot_audio_wave(y_combined, sr, title=\"Audio without Silence\")\n",
    "    plot_audio_wave_spect(y_combined, sr, title=\"Audio without Silence\")\n",
    "    plot_audio_wave_mfcc(y_combined, sr, title=\"Audio without Silence\")\n",
    "    \n",
    "    # Denoising using D1 method\n",
    "    y_denoised_d1 = denoise_method_d1(y_combined)\n",
    "    display(Audio(y_denoised_d1, rate=sr))\n",
    "    plot_audio_wave(y_denoised_d1, sr, title=\"Denoised Audio (D1 Method)\")\n",
    "    plot_audio_wave_spect(y_denoised_d1, sr, title=\"Denoised Audio (D1 Method)\")\n",
    "    plot_audio_wave_mfcc(y_denoised_d1, sr, title=\"Denoised Audio (D1 Method)\")\n",
    "\n",
    "    # Denoising using spectral method\n",
    "    y_denoised_spectral = denoise_method_spectral(y_combined, sr)\n",
    "    display(Audio(y_denoised_spectral, rate=sr))\n",
    "    plot_audio_wave(y_denoised_spectral, sr, title=\"Denoised Audio (Spectral Method)\")\n",
    "    plot_audio_wave_spect(y_denoised_spectral, sr, title=\"Denoised Audio (Spectral Method)\")\n",
    "    plot_audio_wave_mfcc(y_denoised_spectral, sr, title=\"Denoised Audio (Spectral Method)\")\n",
    "\n",
    "    # Extracting MFCCs for raw audio\n",
    "    mfcc_raw = lib.feature.mfcc(y=y_combined, sr=sr, n_mfcc=20)\n",
    "    mean_mfcc_raw = mfcc_raw.mean(axis=1)\n",
    "    data_raw.append([*mean_mfcc_raw, speaker])\n",
    "\n",
    "    # Extracting MFCCs for processed (denoised) audio\n",
    "    mfcc_processed = lib.feature.mfcc(y=y_denoised_spectral, sr=sr, n_mfcc=20)\n",
    "    mean_mfcc_processed = mfcc_processed.mean(axis=1)\n",
    "    data_processed.append([*mean_mfcc_processed, speaker])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7473813-2aee-4d2c-ac1d-f641cdb53a63",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#correlation Matrix\n",
    "\n",
    "def merge_audios(audio_files):\n",
    "    audios = [lib.load(file, sr=None)[0] for file in audio_files]\n",
    "    return np.concatenate(audios)\n",
    "   \n",
    "\n",
    "data_raw = []\n",
    "data_processed = []\n",
    "\n",
    "base_path = \"16000_pcm_speeches\"\n",
    "speakers = [\"Benjamin_Netanyau\", \"Jens_Stoltenberg\", \"Julia_Gillard\", \"Magaret_Tarcher\", \"Nelson_Mandela\"]\n",
    "\n",
    "def merge_audios(audio_files):\n",
    "    audios = [lib.load(file, sr=None)[0] for file in audio_files]\n",
    "    return np.concatenate(audios)\n",
    "\n",
    "def denoise_method_d1(y):\n",
    "    D = lib.stft(y)\n",
    "    magnitude, phase = lib.magphase(D)\n",
    "    magnitude_denoised = lib.decompose.nn_filter(magnitude, aggregate=np.median, metric='cosine')\n",
    "    y_denoised = lib.istft(magnitude_denoised * phase)\n",
    "    return y_denoised\n",
    "\n",
    "def denoise_method_spectral(y, sr):\n",
    "    # Calculate spectral centroid\n",
    "    cent = np.mean(lib.feature.spectral_centroid(y=y, sr=sr))\n",
    "    \n",
    "    # Low-shelf\n",
    "    sos = iirfilter(N=2, Wn=cent/sr, btype='low', ftype='butter', output='sos')\n",
    "    y_low_shelf = sosfilt(sos, y)\n",
    "    \n",
    "    # High-shelf\n",
    "    sos = iirfilter(N=2, Wn=cent/sr, btype='high', ftype='butter', output='sos')\n",
    "    y_high_shelf = sosfilt(sos, y_low_shelf)\n",
    "    \n",
    "    # Amplify signal to compensate for volume reduction\n",
    "    y_denoised = y_high_shelf * 10\n",
    "    return y_denoised\n",
    "\n",
    "for speaker in speakers:\n",
    "    speaker_path = os.path.join(base_path, speaker)\n",
    "    audio_files = [os.path.join(speaker_path, f) for f in sorted(os.listdir(speaker_path))]\n",
    "    \n",
    "    y, sr = merge_audios(audio_files[:10]), 16000\n",
    "\n",
    "\n",
    "    # Trimming\n",
    "    y_trimmed, _ = lib.effects.trim(y, top_db=20)\n",
    "\n",
    "    # Removing silence\n",
    "    y_no_silence = [y_trimmed[start:end] for start, end in lib.effects.split(y_trimmed, top_db=20)]\n",
    "    y_combined = np.concatenate(y_no_silence)\n",
    "\n",
    "    # Denoising using D1 method\n",
    "    y_denoised_d1 = denoise_method_d1(y_combined)\n",
    "\n",
    "\n",
    "    # Denoising using spectral method\n",
    "    y_denoised_spectral = denoise_method_spectral(y_combined, sr)\n",
    "\n",
    "    \n",
    "     # Extracting MFCCs for raw audio\n",
    "    mfcc_raw = lib.feature.mfcc(y=y_combined, sr=sr, n_mfcc=20)\n",
    "    mean_mfcc_raw = mfcc_raw.mean(axis=1)\n",
    "    data_raw.append([*mean_mfcc_raw, speaker])\n",
    "    \n",
    "    # Extracting MFCCs for processed (denoised) audio\n",
    "    mfcc_processed = lib.feature.mfcc(y=y_denoised_spectral, sr=sr, n_mfcc=20)\n",
    "    mean_mfcc_processed = mfcc_processed.mean(axis=1)\n",
    "    data_processed.append([*mean_mfcc_processed, speaker])\n",
    "\n",
    "    \n",
    "\n",
    "# Raw Audio MFCCs\n",
    "df_raw = pd.DataFrame(data_raw, columns=[f\"{i+1}\" for i in range(20)] + ['Label'])\n",
    "mean_mfccs_raw = df_raw.groupby('Label').mean()\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(mean_mfccs_raw, annot=True, cmap=\"YlGnBu\", linewidths=.5)\n",
    "plt.title(\"Mean MFCCs for Raw Audio of each speaker\")\n",
    "plt.show()\n",
    "\n",
    "# Processed Audio MFCCs\n",
    "df_processed = pd.DataFrame(data_processed, columns=[f\"{i+1}\" for i in range(20)] + ['Label'])\n",
    "mean_mfccs_processed = df_processed.groupby('Label').mean()\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(mean_mfccs_processed, annot=True, cmap=\"YlGnBu\", linewidths=.5)\n",
    "plt.title(\"Mean MFCCs for Processed Audio of each speaker\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation Matrix for Raw Audio MFCCs\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df_raw.iloc[:, :-1].corr(), annot=True, cmap=\"coolwarm\", linewidths=.5)\n",
    "plt.title(\"MFCCs Correlation Matrix for Raw Audio\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation Matrix for Processed Audio MFCCs\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df_processed.iloc[:, :-1].corr(), annot=True, cmap=\"coolwarm\", linewidths=.5)\n",
    "plt.title(\"MFCCs Correlation Matrix for Processed Audio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58564ac0-c1a6-4376-b2c1-3f795784c7d0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b9b239b-6067-4018-9dbf-4aa9e10ad2d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Second Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd32902-d39d-4ce3-b941-8ce2414e0b1c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def trim_silence(audio):\n",
    "    y_trimmed, index = librosa.effects.trim(audio, top_db=20)\n",
    "    return y_trimmed\n",
    "\n",
    "def random_5sec_snippet(audio, sample_rate=16000):\n",
    "    num_samples_for_5sec = 5 * sample_rate\n",
    "    if len(audio) < num_samples_for_5sec:\n",
    "        return audio\n",
    "    start_idx = np.random.randint(0, len(audio) - num_samples_for_5sec)\n",
    "    return audio[start_idx:start_idx + num_samples_for_5sec]\n",
    "\n",
    "base_path = \"16000_pcm_speeches/\"\n",
    "all_wavs = []\n",
    "all_labels = []\n",
    "\n",
    "# Loading speaker wavs\n",
    "for speaker in os.listdir(base_path):\n",
    "    if os.path.isdir(base_path + speaker) and speaker != \"background_noise\" and speaker != \"other\":\n",
    "        for file in os.listdir(base_path + speaker):\n",
    "            wav, _ = librosa.load(base_path + speaker + '/' + file, sr=16000)\n",
    "            all_wavs.append([wav, len(wav) / 16000])\n",
    "            all_labels.append(speaker)\n",
    "\n",
    "# Loading noise wavs\n",
    "noise_wavs = []\n",
    "noise_dir = base_path + \"_background_noise_/\"\n",
    "for file in os.listdir(noise_dir):\n",
    "    wav, _ = librosa.load(noise_dir + file, sr=16000)\n",
    "    noise_wavs.append(wav)\n",
    "\n",
    "def get_noise_from_sound(signal, noise, SNR):\n",
    "    RMS_s = np.sqrt(np.mean(signal ** 2))\n",
    "    RMS_n = np.sqrt(RMS_s ** 2 / (pow(10, SNR / 10)))\n",
    "    RMS_current = np.sqrt(np.mean(noise ** 2))\n",
    "    noise = noise * (RMS_n / RMS_current)\n",
    "    return noise\n",
    "\n",
    "noisy_dataset = []\n",
    "for i in range(len(all_wavs)):\n",
    "    signal = np.interp(all_wavs[i][0], (all_wavs[i][0].min(), all_wavs[i][0].max()), (-1, 1))\n",
    "    \n",
    "    noise_idx = np.random.randint(0, len(noise_wavs))\n",
    "    noise = noise_wavs[noise_idx]\n",
    "\n",
    "    if len(signal) < len(noise):\n",
    "        noise = get_noise_from_sound(signal, noise, np.random.uniform(0, 10))\n",
    "        noise = noise[:len(signal)]\n",
    "    else:\n",
    "        signal = signal[:len(noise)]\n",
    "        noise = get_noise_from_sound(signal, noise, np.random.uniform(0, 10))\n",
    "    noisy = signal + noise\n",
    "    noisy = random_5sec_snippet(noisy)\n",
    "    noisy_dataset.append([noisy, all_labels[i]])\n",
    "    if i % 200 == 0:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.waveshow(noisy, sr=16000)\n",
    "        plt.title(f\"Noisy Signal for {i}th Audio\")\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.show()\n",
    "\n",
    "all_mfccs = []\n",
    "for i in range(len(noisy_dataset)):\n",
    "    mfccs = librosa.feature.mfcc(y=noisy_dataset[i][0], sr=16000, n_mfcc=13)\n",
    "    delta = librosa.feature.delta(mfccs)\n",
    "    double_delta = librosa.feature.delta(delta)\n",
    "    combined = np.vstack((mfccs, delta, double_delta))\n",
    "    all_mfccs.append([combined, noisy_dataset[i][1]])\n",
    "    if i == 0:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.specshow(combined, x_axis='time', sr=16000)\n",
    "        plt.colorbar(format=\"%+2.0f dB\")\n",
    "        plt.title(\"MFCCs, Delta, Double Delta\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d7add-44ce-4fad-970f-d27d66057cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b69be-1685-4ed5-a7de-40d160af6745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbca4af-b178-4bfa-8168-956aa5675205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
