{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["# Import the required packages\n","import os\n","import numpy as np \n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import pathlib\n","import librosa.display\n","from tqdm import tqdm\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","\n","import librosa"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["['Benjamin_Netanyau',\n"," 'Jens_Stoltenberg',\n"," 'Julia_Gillard',\n"," 'Magaret_Tarcher',\n"," 'Nelson_Mandela',\n"," 'other',\n"," 'tf_Wav_reader.py',\n"," '_background_noise_']"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Get the data directories\n","data_dir = \"../Speaker-Identification/16000_pcm_speeches/\"\n","os.listdir(data_dir)"]},{"cell_type":"markdown","metadata":{},"source":["# Process training dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["# get wav paths\n","def get_wav_paths(speaker):\n","    speaker_path = data_dir + speaker\n","    all_paths = [item for item in os.listdir(speaker_path)]\n","    return all_paths"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["nelson_mandela_paths = get_wav_paths(\"Nelson_Mandela\")\n","margaret_thatcher_paths = get_wav_paths(\"Magaret_Tarcher\")\n","benjamin_netanyau_paths = get_wav_paths(\"Benjamin_Netanyau\")\n","jens_stoltenberg_paths = get_wav_paths( 'Jens_Stoltenberg')\n","julia_gillard_paths = get_wav_paths(\"Julia_Gillard\")\n","\n","noise1_paths = get_wav_paths(\"_background_noise_\")\n","noise2_paths = get_wav_paths(\"other\")"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["# load the data\n","def load_wav(wav_path, speaker):\n","    with tf.compat.v1.Session(graph=tf.compat.v1.Graph()) as sess:\n","        wav_path = data_dir + speaker + \"/\" + wav_path\n","        wav_filename_placeholder = tf.compat.v1.placeholder(tf.compat.v1.string, [])\n","        wav_loader = tf.io.read_file(wav_filename_placeholder)\n","        wav_decoder = tf.audio.decode_wav(wav_loader, desired_channels=1)\n","        wav_data = sess.run(\n","            wav_decoder, feed_dict={\n","                wav_filename_placeholder: wav_path\n","            }).audio.flatten().reshape((1, 16000))\n","        sess.close()\n","    return wav_data"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["# create training data\n","def generate_training_data(speaker_paths, speaker, label):\n","    wavs, labels = [], []\n","    for i in tqdm(speaker_paths):\n","        wav = load_wav(i, speaker)\n","        wavs.append(wav)\n","        labels.append(label)\n","    return wavs, labels\n"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1500/1500 [00:05<00:00, 264.73it/s]\n","100%|██████████| 1500/1500 [00:05<00:00, 270.93it/s]\n","100%|██████████| 1500/1500 [00:05<00:00, 273.15it/s]\n","100%|██████████| 1500/1500 [00:05<00:00, 274.84it/s]\n","100%|██████████| 1501/1501 [00:05<00:00, 276.45it/s]\n"]}],"source":["nelson_mandela_wavs, nelson_mandela_labels = generate_training_data(nelson_mandela_paths, \"Nelson_Mandela\", 0) \n","margaret_thatcher_wavs, margaret_thatcher_labels = generate_training_data(margaret_thatcher_paths, \"Magaret_Tarcher\", 1) \n","benjamin_netanyau_wavs, benjamin_netanyau_labels = generate_training_data(benjamin_netanyau_paths, \"Benjamin_Netanyau\", 2) \n","jens_stoltenberg_wavs, jens_stoltenberg_labels = generate_training_data(jens_stoltenberg_paths, \"Jens_Stoltenberg\", 3) \n","julia_gillard_wavs, julia_gillard_labels = generate_training_data(julia_gillard_paths, \"Julia_Gillard\", 4) "]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["# remove the extra wav for Julia Gillard\n","julia_gillard_labels = julia_gillard_labels[1:]\n","julia_gillard_wavs = julia_gillard_wavs[1:]"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["all_wavs = nelson_mandela_wavs + margaret_thatcher_wavs + benjamin_netanyau_wavs + jens_stoltenberg_wavs + julia_gillard_wavs\n","all_labels = nelson_mandela_labels + margaret_thatcher_labels + benjamin_netanyau_labels + jens_stoltenberg_labels + julia_gillard_labels"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\dhvan\\AppData\\Local\\Temp\\ipykernel_4976\\1263193249.py:46: WavFileWarning: Chunk (non-data) not understood, skipping it.\n","  fs, noise_file = read(data_dir + '_background_noise_/' + noise)\n"]},{"name":"stdout","output_type":"stream","text":["0\n","200\n","400\n","600\n","800\n","1000\n","1200\n","1400\n","1600\n","1800\n","2000\n","2200\n","2400\n","2600\n","2800\n","3000\n","3200\n","3400\n","3600\n","3800\n","4000\n","4200\n","4400\n","4600\n","4800\n","5000\n","5200\n","5400\n","5600\n","5800\n","6000\n","6200\n","6400\n","6600\n","6800\n","7000\n","7200\n","7400\n"]}],"source":["from scipy.io.wavfile import read\n","from scipy.io.wavfile import write\n","from random import randint\n","\n","def cut_random_section(noise2, size2):\n","    size21 = noise2.size\n","    starting_point2 = randint(0,(noise2.size - size2))\n","    end_point2 = starting_point2 + size2\n","    noise_cut_part2 = noise2[starting_point2:end_point2]\n","    return noise_cut_part2\n","\n","def mix(audio1, noise1, snr1):\n","    audio_max = max(audio1)\n","    if audio_max==0:\n","        audio_max = int(np.random.uniform(0.7,1)*32767)\n","    audio1 = audio1*1.\n","    audio1 = audio1/audio_max\n","    noise1 = cut_random_section(noise1, audio1.size)\n","    noise1 = noise1*1.\n","    noise1 = noise1/max(noise1)\n","    gain = pow(10,(snr1/10.))\n","    numerator = np.mean(abs(audio1)**2)\n","    denominator = numerator/gain\n","    noise_power = np.mean(abs(noise1)**2)\n","    mult_value = (denominator/noise_power)**0.5\n","    noisy1 = audio1 + noise1*mult_value\n","    if max(audio1)==0:\n","        noisy1 = noise1\n","    else:    \n","        noisy1 = noisy1/max(noisy1)\n","    noisy1 = np.array(noisy1*audio_max, dtype='int16')\n","    return noise1*mult_value, mult_value, noisy1\n","\n","noise_wavs = []\n","noise_labels = []\n","snr_dB = 10\n","for i in range(len(all_wavs)):\n","    for noise in os.listdir(data_dir + 'other'):\n","        fs, noise_file = read(data_dir + 'other/' + noise)\n","        x = all_wavs[i][0]\n","        noise_temp, mult_value, noisy = mix(x, noise_file, snr_dB)\n","        if noisy.any() != 0:\n","            noise_wavs.append(noisy)\n","            noise_labels.append(all_labels[i])\n","    for noise in os.listdir(data_dir + '_background_noise_'):\n","        fs, noise_file = read(data_dir + '_background_noise_/' + noise)\n","        x = all_wavs[i][0]\n","        if len(noise_file.shape) > 1:\n","            noise_file = np.reshape(noise_file, (noise_file.shape[0]*noise_file.shape[1]))\n","        noise_temp, mult_value, noisy = mix(x, noise_file, snr_dB)\n","        if noisy.any() != 0:\n","            noise_wavs.append(noisy)\n","            noise_labels.append(all_labels[i]) \n","    if i%200 == 0:\n","        print(i)"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(8101, 16000) (8101,)\n"]}],"source":["for i in range(len(all_wavs)):\n","    noise_labels.append(all_labels[i])\n","    noise_wavs.append(all_wavs[i][0])\n","final_wavs = np.array(noise_wavs)\n","final_labels = np.array(noise_labels)\n","\n","print(final_wavs.shape, final_labels.shape)"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["# split the dataset into trainin and testing set\\\n","train_wavs, test_wavs, train_labels, test_labels = train_test_split(final_wavs, final_labels, test_size=0.1)"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["train_x, train_y = np.array(train_wavs), np.array(train_labels)\n","test_x, test_y = np.array(test_wavs), np.array(test_labels)"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[],"source":["train_y = tf.keras.utils.to_categorical(train_y)\n","test_y = tf.keras.utils.to_categorical(test_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import csv\n","# with open('train_x.csv', 'wt') as f:\n","#     csv_writer = csv.writer(f, quoting=csv.QUOTE_NONE)\n","#     csv_writer.writerows(train_x)\n","# with open('test_x.csv', 'wt') as f:\n","#     csv_writer = csv.writer(f, quoting=csv.QUOTE_NONE)\n","#     csv_writer.writerows(test_x)\n","# with open('train_y.csv', 'wt') as f:\n","#     csv_writer = csv.writer(f, quoting=csv.QUOTE_NONE)\n","#     csv_writer.writerows(train_y)\n","# with open('test_y.csv', 'wt') as f:\n","#     csv_writer = csv.writer(f, quoting=csv.QUOTE_NONE)\n","#     csv_writer.writerows(test_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# MFCC Feature Extraction\n","\n","train_x_new = []\n","test_x_new = []\n","INPUT_SHAPE = (126,40)\n","\n","train_x_new = np.zeros((train_x.shape[0], INPUT_SHAPE[0], INPUT_SHAPE[1]), dtype=np.float64)\n","\n","count = 0\n","for sample in train_x:\n","    mfcc = librosa.feature.mfcc(y=sample, sr=16000, hop_length=128, n_fft=256, n_mfcc=20)\n","    mfcc_delta = librosa.feature.delta(mfcc)[:10, :]\n","    mfcc_double_delta = librosa.feature.delta(mfcc, order=2)[:10, :]\n","    train_x_new[count, :, :20] = mfcc.T\n","    train_x_new[count, :, 20:30] = mfcc_delta.T\n","    train_x_new[count, :, 30:] = mfcc_double_delta.T\n","    count += 1\n","    if count%500 == 0:\n","        print('Train', count)\n","        \n","test_x_new = np.zeros((test_x.shape[0], INPUT_SHAPE[0], INPUT_SHAPE[1]), dtype=np.float64)\n","\n","count = 0\n","for sample in test_x:\n","    mfcc = librosa.feature.mfcc(y=sample, sr=16000, hop_length=128, n_fft=256, n_mfcc=20)\n","    mfcc_delta = librosa.feature.delta(mfcc)[:10, :]\n","    mfcc_double_delta = librosa.feature.delta(mfcc, order=2)[:10, :]\n","    test_x_new[count, :, :20] = mfcc.T\n","    test_x_new[count, :, 20:30] = mfcc_delta.T\n","    test_x_new[count, :, 30:] = mfcc_double_delta.T\n","    count += 1\n","    if count%500 == 0:\n","        print('Test', count)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_x_new = np.expand_dims(train_x_new, axis=3)\n","test_x_new = np.expand_dims(test_x_new, axis=3)\n","print(train_x_new.shape, test_x_new.shape)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
